{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extractive and Abstractive Text Summarisation\n",
    "\n",
    "Text summarization can be done using multiple ways however all the algorithms can be categorized under extractive or abstractive methods. Extractive methods essentially use the same words in the text while Abstractive methods attempt to extract content which may not be directly present in the text. I have attempted to use text rank algorithm for extractive methods and LSTM based encoder decoder architecture for abstractive methods.\n",
    "The data set used for text extraction is a Kaggle dataset available at https://www.kaggle.com/sunnysai12345/news-summary\n",
    "\n",
    "Please note that exractive text summariser takes about 20 minutes to generate the summary however abstractive deep learning summariser takes close to 24 hours to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hrishekesh.shinde/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from string import punctuation\n",
    "from heapq import nlargest\n",
    "import gensim\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Activation, RepeatVector, Input, \\\n",
    "concatenate, Permute, Flatten, Multiply, TimeDistributed, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import LambdaCallback\n",
    "from keras.models import Model\n",
    "import math\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>headlines</th>\n",
       "      <th>read_more</th>\n",
       "      <th>text</th>\n",
       "      <th>ctext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Chhavi Tyagi</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Daman &amp; Diu revokes mandatory Rakshabandhan in...</td>\n",
       "      <td>http://www.hindustantimes.com/india-news/raksh...</td>\n",
       "      <td>The Administration of Union Territory Daman an...</td>\n",
       "      <td>The Daman and Diu administration on Wednesday ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Daisy Mowke</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Malaika slams user who trolled her for 'divorc...</td>\n",
       "      <td>http://www.hindustantimes.com/bollywood/malaik...</td>\n",
       "      <td>Malaika Arora slammed an Instagram user who tr...</td>\n",
       "      <td>From her special numbers to TV?appearances, Bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Arshiya Chopra</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>'Virgin' now corrected to 'Unmarried' in IGIMS...</td>\n",
       "      <td>http://www.hindustantimes.com/patna/bihar-igim...</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "      <td>The Indira Gandhi Institute of Medical Science...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Sumedha Sehra</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Aaj aapne pakad liya: LeT man Dujana before be...</td>\n",
       "      <td>http://indiatoday.intoday.in/story/abu-dujana-...</td>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "      <td>Lashkar-e-Taiba's Kashmir commander Abu Dujana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Aarushi Maheshwari</td>\n",
       "      <td>03 Aug 2017,Thursday</td>\n",
       "      <td>Hotel staff to get training to spot signs of s...</td>\n",
       "      <td>http://indiatoday.intoday.in/story/sex-traffic...</td>\n",
       "      <td>Hotels in Maharashtra will train their staff t...</td>\n",
       "      <td>Hotels in Mumbai and other Indian cities are t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0              author                  date  \\\n",
       "0           0        Chhavi Tyagi  03 Aug 2017,Thursday   \n",
       "1           1         Daisy Mowke  03 Aug 2017,Thursday   \n",
       "2           2      Arshiya Chopra  03 Aug 2017,Thursday   \n",
       "3           3       Sumedha Sehra  03 Aug 2017,Thursday   \n",
       "4           4  Aarushi Maheshwari  03 Aug 2017,Thursday   \n",
       "\n",
       "                                           headlines  \\\n",
       "0  Daman & Diu revokes mandatory Rakshabandhan in...   \n",
       "1  Malaika slams user who trolled her for 'divorc...   \n",
       "2  'Virgin' now corrected to 'Unmarried' in IGIMS...   \n",
       "3  Aaj aapne pakad liya: LeT man Dujana before be...   \n",
       "4  Hotel staff to get training to spot signs of s...   \n",
       "\n",
       "                                           read_more  \\\n",
       "0  http://www.hindustantimes.com/india-news/raksh...   \n",
       "1  http://www.hindustantimes.com/bollywood/malaik...   \n",
       "2  http://www.hindustantimes.com/patna/bihar-igim...   \n",
       "3  http://indiatoday.intoday.in/story/abu-dujana-...   \n",
       "4  http://indiatoday.intoday.in/story/sex-traffic...   \n",
       "\n",
       "                                                text  \\\n",
       "0  The Administration of Union Territory Daman an...   \n",
       "1  Malaika Arora slammed an Instagram user who tr...   \n",
       "2  The Indira Gandhi Institute of Medical Science...   \n",
       "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...   \n",
       "4  Hotels in Maharashtra will train their staff t...   \n",
       "\n",
       "                                               ctext  \n",
       "0  The Daman and Diu administration on Wednesday ...  \n",
       "1  From her special numbers to TV?appearances, Bo...  \n",
       "2  The Indira Gandhi Institute of Medical Science...  \n",
       "3  Lashkar-e-Taiba's Kashmir commander Abu Dujana...  \n",
       "4  Hotels in Mumbai and other Indian cities are t...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dataset downloaded from kaggle - https://www.kaggle.com/sunnysai12345/news-summary\n",
    "df_small = pd.read_csv('./news_summary.csv', encoding='latin1')\n",
    "# utilize only first 700 rows of dataframe because of long execution time.\n",
    "df_small = df_small.head(700)\n",
    "df_small.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity score\n",
    "\n",
    "I have used the concept of synsets to calculate similarity between two sets. The similarity score is used as a metric of comparision between two texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function to generate pos tags for text\n",
    "def generate_pos_tags(text):\n",
    "    pos_text = nltk.pos_tag(nltk.word_tokenize(text.lower()))\n",
    "    return pos_text\n",
    "\n",
    "# utility function to convert penn tree bank tag to wordnet\n",
    "def penn_to_wn(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    "    \n",
    "    if tag.startswith('N'):\n",
    "        return 'n'\n",
    "    \n",
    "    if tag.startswith('R'):\n",
    "        return 'r'\n",
    " \n",
    "    if tag.startswith('V'):\n",
    "        return 'v'\n",
    " \n",
    "    return None\n",
    "\n",
    "# get synset from wordnet\n",
    "def get_synset(word, tag):\n",
    "    wn_tag = penn_to_wn(tag)\n",
    "    if wn_tag is None:\n",
    "        return None\n",
    "    try:\n",
    "        return wn.synsets(word, wn_tag)[0]\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source https://www.aaai.org/Papers/AAAI/2006/AAAI06-123.pdf\n",
    "def calculate_similarity(text1, text2):\n",
    "    '''Utility function to calculate similarity'''\n",
    "    text1 = generate_pos_tags(text1)\n",
    "    text2 = generate_pos_tags(text2)\n",
    "    \n",
    "    synsets1 = [get_synset(*tagged_word) for tagged_word in text1]\n",
    "    synsets2 = [get_synset(*tagged_word) for tagged_word in text2]\n",
    " \n",
    "    # Filter out the Nones\n",
    "    synsets1 = [ss for ss in synsets1 if ss]\n",
    "    synsets2 = [ss for ss in synsets2 if ss]\n",
    " \n",
    "    score, count = 0.0, 0\n",
    " \n",
    "    for synset in synsets1:\n",
    "        similarity_values = []\n",
    "        \n",
    "        best_score = 0.0\n",
    "        for ss in synsets2:\n",
    "            similarity = synset.path_similarity(ss)\n",
    "            if similarity is not None:\n",
    "                similarity_values.append(similarity)\n",
    "            if len(similarity_values):\n",
    "                # take the max similarity score\n",
    "                best_score = max(similarity_values)\n",
    "        if best_score > 0.0:\n",
    "            score += best_score\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        # calculate the average similarity score\n",
    "        score /= count\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assuming corp is a dataframe which has columns with names 'text' and 'ctext'\n",
    "def generate_summary_and_similarity_score(corp, corp_summarizer, summariser_name):\n",
    "    '''This function joins the final summary and calculates the similarity score.\n",
    "    It further adds the summary and the similarity score to the dataset. \n",
    "    Please note that this function only joins the summary'''\n",
    "    summary_list = []\n",
    "    similarity_list = []\n",
    "    for ctext, text in zip(corp['ctext'], corp['text']):\n",
    "        summary = ''\n",
    "        similarity = 0.0\n",
    "        if ctext is not None and str(ctext) != 'NA':\n",
    "            summary = summary.join(corp_summarizer(ctext))\n",
    "            similarity = calculate_similarity(summary, str(text))\n",
    "        summary_list.append(summary)\n",
    "        similarity_list.append(similarity)\n",
    "    corp['summary-'+summariser_name] = summary_list\n",
    "    corp['similarity-score-'+summariser_name] = similarity_list\n",
    "\n",
    "# using dissimilarity score like a loss function - minimise dissimilarity to minimise loss        \n",
    "def get_cumulative_similarity_and_dissimilarity_score(corp, summariser_name):\n",
    "    '''Calculate cumulative dissimilarity and similarity scores.'''\n",
    "    similarity = 0.0\n",
    "    dissimilarity = 0.0\n",
    "    for similarity_score in corp['similarity-score-'+summariser_name]:\n",
    "        if similarity_score is not None:\n",
    "            similarity+= similarity_score\n",
    "            # dissimilarity score is assumed to be 1 - similarity score for each row\n",
    "            dissimilarity+= (1 - similarity_score)\n",
    "    return dissimilarity, similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On inspecting the dataset we find that in many cases, there is no space between \n",
    "# full point at the end of one sentence and the begining of the next sentence\n",
    "def preprocess_data(corp):\n",
    "    '''Preprocess the dataset'''\n",
    "    sanitized_text = []\n",
    "    corp['headlines'].fillna('NA')\n",
    "    corp['text'].fillna('NA')\n",
    "    corp['ctext'].fillna('NA')\n",
    "    for ctext in corp['ctext']:\n",
    "        ctext_str = re.sub(r'\\.', '. ',str(ctext))\n",
    "        ctext_str = re.sub(r'don\\'t', 'do not',str(ctext))\n",
    "        ctext_str = re.sub(r'isn\\'t', 'is not',str(ctext))\n",
    "        ctext_str = re.sub(r'won\\'t', 'will not',str(ctext))\n",
    "        ctext_str = re.sub(r'i\\'m', 'i am',str(ctext))\n",
    "        ctext_str = re.sub(r'who\\'re', 'who are',str(ctext))\n",
    "        ctext_str = re.sub(r'all\\'s', 'all is',str(ctext))\n",
    "        ctext_str = re.sub(r'couldn\\'t', 'could not',str(ctext))\n",
    "        ctext_str = re.sub(r'you\\'d', 'you would',str(ctext))\n",
    "        ctext_str = re.sub(r'don\\'ts', 'donts',str(ctext))\n",
    "        ctext_str = re.sub(r'they\\'d', 'they would',str(ctext))\n",
    "        ctext_str = re.sub(r'b\\'coz', 'because',str(ctext))\n",
    "        ctext_str = re.sub(r'do\\'s', 'dos',str(ctext))\n",
    "        ctext_str = re.sub(r'you\\'re', 'you are',str(ctext))\n",
    "        ctext_str = re.sub(r'he\\'d', 'he would',str(ctext))\n",
    "        ctext_str = re.sub(r'aren\\'t', 'are not',str(ctext))\n",
    "        ctext_str = re.sub(r'hasn\\'t', 'has not',str(ctext))\n",
    "        ctext_str = re.sub(r'he\\'ll', 'he will',str(ctext))\n",
    "        ctext_str = re.sub(r'aren\\'t', 'are not',str(ctext))\n",
    "        ctext_str = re.sub(r'ain\\'t', 'am not',str(ctext))\n",
    "        ctext_str = re.sub(r'i\\'d', 'i would',str(ctext))\n",
    "        ctext_str = re.sub(r'they\\'re', 'they are',str(ctext))\n",
    "        ctext_str = re.sub(r'she\\'d', 'she would',str(ctext))\n",
    "        ctext_str = re.sub(r'wouldn\\'t', 'would not',str(ctext))\n",
    "        ctext_str = re.sub(r'y\\'all', 'you',str(ctext))\n",
    "        ctext_str = re.sub(r'they\\'ll', 'they will',str(ctext))\n",
    "        ctext_str = re.sub(r'would\\'ve', 'would have',str(ctext))\n",
    "        ctext_str = re.sub(r'you\\'ll', 'you will',str(ctext))\n",
    "        ctext_str = re.sub(r'weren\\'t', 'were not',str(ctext))\n",
    "        ctext_str = re.sub(r'ma\\'am', 'madam',str(ctext))\n",
    "        ctext_str = re.sub(r'didn\\'t', 'did not',str(ctext))\n",
    "        ctext_str = re.sub(r'hon\\'ble', 'honorable',str(ctext))\n",
    "        ctext_str = re.sub(r'it\\'ll', 'it will',str(ctext))\n",
    "        ctext_str = re.sub(r'li\\'l', 'little',str(ctext))\n",
    "        ctext_str = re.sub(r'i\\'ll', 'i will',str(ctext))\n",
    "        ctext_str = re.sub(r'\\'', '',str(ctext))\n",
    "        sanitized_text.append(ctext_str.lower())\n",
    "    corp['ctext'] = sanitized_text\n",
    "\n",
    "preprocess_data(df_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Extraction using text rank algorithm\n",
    "\n",
    "Text rank algorithm uses the word frequency distribution to get the most common words and hence the sentences which have these common words. This algorithm does a good job tosummarise the content. I tried to use the comparison of similarity and dissimilarity score to indicate whether the summary is effective. For a dataset like which has news articles, this is a very effective method. This model took about 20 minutes to generate the summary for all texts in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(316.25684940943626, 383.7431505905638)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A: Extractive algorithms: Summariser based on word frequency - text rank algorithm\n",
    "def get_word_freq(ctext):\n",
    "    '''Function to get the word frequency distribution'''\n",
    "    words = nltk.word_tokenize(str(ctext).lower())\n",
    "    word_list = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english') and word not in punctuation:\n",
    "            word_list.append(word)\n",
    "    freq = nltk.FreqDist(word_list)\n",
    "    return freq             \n",
    "\n",
    "get_word_freq(df_small)\n",
    "\n",
    "# get top 3 sentences with 5 most common words\n",
    "def summarize_word_frequencies(ctext):\n",
    "    '''This function uses the text rank algorithm to extract the summary'''\n",
    "    sentences = nltk.sent_tokenize(str(ctext).lower())\n",
    "    sent_scores = defaultdict(int)\n",
    "    common_words = []\n",
    "    fdist = get_word_freq(ctext)\n",
    "    # get the 5 most frequent words in the text\n",
    "    for word_freq in fdist.most_common(5):\n",
    "        common_words.append(word_freq[0])\n",
    "    sent_count = -1\n",
    "    # get the top 3 sentences based on word frequency distribution of most common words\n",
    "    for sentence in sentences:\n",
    "        sent_count += 1\n",
    "        sent_score = 0\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            if word in common_words:\n",
    "                sent_score += fdist[word]\n",
    "        # scores are calculated for all sentences    \n",
    "        sent_scores[sent_count] = sent_score\n",
    "    # top 3 sentences based on scores are selected as the summary\n",
    "    imp_sents_index = nlargest(3, sent_scores, key=sent_scores.get)\n",
    "    imp_sents_list = []\n",
    "    for index in imp_sents_index:\n",
    "        imp_sents_list.append(sentences[index])\n",
    "    return ' '.join(imp_sents_list)\n",
    "        \n",
    "generate_summary_and_similarity_score(df_small, summarize_word_frequencies, 'word-freq')\n",
    "# printed the scores for comparison\n",
    "get_cumulative_similarity_and_dissimilarity_score(df_small, 'word-freq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Summariser using Abstractive deep learning methods\n",
    "\n",
    "Abstractive summarization will help to get more than just summary of the text. The intention is to capture the meaning of the text to a certain extent. I have used the following approach for the same:\n",
    "1. Generate vectors of the available corpus using word2vec\n",
    "2. Split data set into train and test sets\n",
    "3. Use LSTM based encoder / decoder model using 2 encoder layers and one decoder layer - I also tried to use attention based model however was unable to integrate the attention layer in the model\n",
    "4. Train the model and generate the summary\n",
    "\n",
    "Although I expected this to work however the model generated summary which was unrelated to the given text and was  gramatically incorrect. Also this takes a very long time to train. I had to run the model for more than 24 hours to complete 100 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4426\n",
      "90\n",
      "-----> Original article\n",
      "in dramatic developments in bihar, nitish kumar on wednesday resigned as chief minister, dumping the rjd and congress to stitch a new alliance with bjp, which quickly announced support to a new government under him. kumar, whose resignation was immediately accepted by governor keshari nath tripathi, will take oath at the chief minister at 5pm on thursday.?in the circumstances that prevail in bihar, it became difficult to run the grand alliance government,? kumar told reporters outside raj bhavan after submitting his resignation to governor keshri nath tripathi.prime minister narendra modi hailed kumar?s resignation, saying in doing so he has joined the fight against corruption. immediately after kumar announced his resignation, modi tweeted: ?congratulations! mr nitish kumar for joining the fight against corruption.?here are wednesday?s highlights on the political crisis in bihar:12.30am: lalu prasad says: ?nitish kumar has underlined that he is an opportunist. having done what he has, he cannot stake claim without a challenge. the congress, cpi-ml and independents will back our claims to form the next government... struggle is my life. i have to be in ranchi for the next three days. however, tejashwi has been asked to present the claim to the governor in the forenoon tomorrow.?12.28am: tejashwi yadav along with rjd delegation will meet bihar governor kn tripathi at 11 am on thursday; to stake claim for forming government.12.07am: taking a letter of support to governor to stake claim for forming government: prem kumar, bjp, on sushil modi and nitish kumar?s meet with governor keshari nath tripathi.12.04am: have asked governor for time to meet, being the largest party we will stake claim to from government, tweets tejashwi yadav. we will also get support from jdu mlas, he says.11.56pm: nitish kumar, sushil modi and bjp, jdu leaders leave for raj bhavan to meet governor keshari nath tripathi.11.11pm: governor keshari nath tripathi reaches raj bhavan after being discharged from hospital.10.36pm: nitish kumar to take oath as cm of bihar at 5pm tomorrow.10.31pm: bihar governor kn tripathi admitted to indira gandhi institute of medical sciences in patna due to an ent problem.10.20pm: nitish may take oath on thursday. a meeting of jd(u) and bjp mlas is underway.10.07pm: when nitish kumar broke alliance with bjp in 2013 he choreographed and followed the same script to secure himself. he surrenders as per needs: lalu prasad?s tweet9.55pm: sushil modi says bjp has handed over a letter of support to the governor, and will be part of the new government. all nda members are heading to 1, aney marg to elect nitish kumar as the leader, modi adds.9.49pm: sushil modi, nityanand rai and other bjp mlas reach nitish kumar?s residence in patna for a meeting of jdu-bjp mlas, reports ani.9.30pm: sushil kumar modi says bjp will be part of the government under nitish kumar. bjp and jd(u) leaders are headed to the raj bhawan to meet governor keshari nath tripathi.9.02pm: ?there are charges against both nitish and tejashwi. the mlas of the grand alliance parties should sit down and pick a new leader. bihar?s poor, deprived and reservation-backed sections gave the grand alliance historic majority against the bjp. this is foeticide of the grand alliance? there were corruption allegations against us before. did nitish kumar not know while making an alliance and forming a government?? -- lalu prasad?s tweets9.01pm: nityanand rai and i called nitish ji. the bjp decides to support him; we will support the government formed under him. we will inform the governor also: ani?quotes sushil modi as saying9pm: thank pm modi for his tweet on my decision, tweets nitish kumar after resigning as bihar cm8.55pm: there?s involvement of narendra modi. nitish kumar slapped people of bihar. people like sushil modi are eager to form a government in bihar: lalu prasad8.40pm: meeting underway at lalu prasad?s residence in patna. rabri devi, tejashwi yadav, tej pratap yadav and senior rjd leaders present.8.23pm: cp joshi was informed at the governor?s house; he asked kumar to wait. joshi said he will get back after a word with rahul gandhi and sonia gandhi: ani quotes sources8.21pm: nitish kumar did not inform rahul gandhi about his intention to resign in their meeting: ani?quotes sources.8.02pm: i appeal to nitish kumar... all rjd, jd(u) and congress mlas should sit down and pick new leader and make a government... his unwillingness to do so will prove he has already struck a deal with the bjp: lalu prasad7.59pm: the bjp welcomes the battle against corruption in bihar; not in favour of midterm elections... a three-member panel from bihar will report to central leadership for final call: jp nadda after the bjp?s parliamentary board meeting.7.58pm: we are deeply disappointed by the news of nitish kumar?s resignation. the congress respects nitish kumar as a leader: randeep surjewala, congress7.57pm: nitish kumar knew that he is accused of section 302; one of india?s cm is main accused in murder and arms case : lalu prasad7.51pm: nitish kumar had said that he will never join hands with bjp: lalu prasad7.50pm: rjd chief lalu prasad says did not speak to the press over corruption allegations against  tejashwi following lawyers? advice. 7.49pm: nitish kumar did not ask for resignation: rjd chief lalu prasad at a press conference after nitish kumar quits as bihar cm.7.45pm: rjd chief lalu prasad to address a press conference shortly.7.42pm: we are happy that bihar cm didn?t compromise on issue of corruption and did not kneel down before rjd: bjp leader sushil kumar modi7.36pm: we don?t want midterm elections in bihar. the mlas should complete their term. a three-member committee will talk to mlas and apprise the centre of the situation in the state: bjp leader sushil kumar modi7.26pm: source say bjp legislature party to finalise support without preconditions at its meeting late evening today.7.25pm: nitish-lalu alliance was a mismatch. this had to happen: union minister ravi shankar prasad on nitish kumar?s resignation7.24pm: pm narendra modi arrives for bjp parliamentary meeting in delhi.7.09pm: the country and time require that for the bright future of india, and especially bihar, people need to rise above political disputes and fight against corruption together: prime minister narendra modi7.09pm: i congratulate nitish kumar for joining the fight against corruption. the 1.25 billion people of the country are welcoming and supporting his honesty: prime minister narendra modi7.08pm: before resigning, i had informed laluji and congress bihar in-charge cp joshi: nitish kumar.7.05pm: gandhiji always said needs can be met on the earth, but greed can never be fulfilled: nitish kumar7.04pm: i am not blaming anyone, those who want to blame me are free to do so: nitish kumar7.02pm: i felt that for a person like me it was not possible to run this government. i don?t want to create a dispute... when i saw there was no other way... i broke myself away (from the alliance with the rjd). the governor has accepted my resignation, and asked me to continue working till the next development.6.58pm: we worked in the alliance till the point we could: nitish kumar6.57pm: if there are charges then they must have replied to it. we felt they were not in a position to speak: nitish kumar6.56pm: it?s not possible to work in this environment; i tried. tried to find out a solution: nitish kumar6.52pm: the situation is such that it?s not possible to work. whatever we do, the discussion is only about one thing: nitish kumar6.50pm: not possible to work in the grand alliance. we made all attempts. we never asked for anyone?s resignation: nitish kumar6.34pm: bihar cm nitish kumar reaches raj bhavan to meet governor keshari nath tripathi amid speculation that he may resign.6.33pm: bihar cm nitish kumar on his way to raj bhavan to meet governor keshari nath tripathi.\n",
      "------> Given Summary\n",
      "Expressing support for Bihar CM Nitish Kumar after his resignation, PM Narendra Modi tweeted, \"Congratulations to Nitish Kumar for joining the fight against corruption.\" He further added that the 125 crore citizens of the country welcome and support his honesty. Kumar had submitted his resignation to Governor Keshari Nath Tripathi citing a crisis in the ruling Congress-RJD-JD(U) alliance.\n",
      "WARNING:tensorflow:From /Users/hrishekesh.shinde/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/hrishekesh.shinde/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/hrishekesh.shinde/anaconda3/envs/tensorflow_cpu/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      "490/490 [==============================] - 134s 274ms/step - loss: 15.0905 - custom_loss: 0.0263\n",
      "Epoch 2/100\n",
      "490/490 [==============================] - 136s 278ms/step - loss: 14.7566 - custom_loss: 0.0263\n",
      "Epoch 3/100\n",
      "490/490 [==============================] - 135s 276ms/step - loss: 14.6083 - custom_loss: 0.0263\n",
      "Epoch 4/100\n",
      "490/490 [==============================] - 136s 278ms/step - loss: 14.5165 - custom_loss: 0.0263\n",
      "Epoch 5/100\n",
      "490/490 [==============================] - 151s 308ms/step - loss: 14.4559 - custom_loss: 0.0263\n",
      "Epoch 6/100\n",
      "490/490 [==============================] - 137s 280ms/step - loss: 14.4302 - custom_loss: 0.0263\n",
      "Epoch 7/100\n",
      "490/490 [==============================] - 150s 306ms/step - loss: 14.4178 - custom_loss: 0.0263\n",
      "Epoch 8/100\n",
      "490/490 [==============================] - 142s 289ms/step - loss: 14.4098 - custom_loss: 0.0263\n",
      "Epoch 9/100\n",
      "490/490 [==============================] - 150s 306ms/step - loss: 14.4008 - custom_loss: 0.0263\n",
      "Epoch 10/100\n",
      "490/490 [==============================] - 138s 283ms/step - loss: 14.3971 - custom_loss: 0.0263\n",
      "Epoch 11/100\n",
      "490/490 [==============================] - 139s 284ms/step - loss: 14.3966 - custom_loss: 0.0263\n",
      "Epoch 12/100\n",
      "490/490 [==============================] - 140s 286ms/step - loss: 14.3945 - custom_loss: 0.0263\n",
      "Epoch 13/100\n",
      "490/490 [==============================] - 142s 289ms/step - loss: 14.3934 - custom_loss: 0.0263\n",
      "Epoch 14/100\n",
      "490/490 [==============================] - 144s 295ms/step - loss: 14.3917 - custom_loss: 0.0263\n",
      "Epoch 15/100\n",
      "490/490 [==============================] - 140s 286ms/step - loss: 14.3920 - custom_loss: 0.0263\n",
      "Epoch 16/100\n",
      "490/490 [==============================] - 141s 288ms/step - loss: 14.3923 - custom_loss: 0.0263\n",
      "Epoch 17/100\n",
      "490/490 [==============================] - 138s 282ms/step - loss: 14.3904 - custom_loss: 0.0263\n",
      "Epoch 18/100\n",
      "490/490 [==============================] - 138s 283ms/step - loss: 14.3884 - custom_loss: 0.0263\n",
      "Epoch 19/100\n",
      "490/490 [==============================] - 140s 286ms/step - loss: 14.3883 - custom_loss: 0.0263\n",
      "Epoch 20/100\n",
      "490/490 [==============================] - 138s 282ms/step - loss: 14.3882 - custom_loss: 0.0263\n",
      "\n",
      " Generating Summary after epoch: 19\n",
      "---->\n",
      "white tweeted find education role himself hours charges complaint face -- wanted 27 passengers trying son claimed meet ministry 1 each resignation deputy health end your sushil raj held night directed along really plan started hours leaders till doing said.the ministers general sent issued rashtrapati raj lot others past saturday traffic deputy due getting tejashwi end great development hours great cm filed pradesh month life team however singh how being being if at or had who are it on on s a that in in that in for a for\n",
      "Epoch 21/100\n",
      "490/490 [==============================] - 137s 280ms/step - loss: 14.3857 - custom_loss: 0.0263\n",
      "Epoch 22/100\n",
      "490/490 [==============================] - 144s 294ms/step - loss: 14.3865 - custom_loss: 0.0263\n",
      "Epoch 23/100\n",
      "490/490 [==============================] - 153s 312ms/step - loss: 14.3891 - custom_loss: 0.0263\n",
      "Epoch 24/100\n",
      "490/490 [==============================] - 149s 305ms/step - loss: 14.3857 - custom_loss: 0.0263\n",
      "Epoch 25/100\n",
      "490/490 [==============================] - 139s 283ms/step - loss: 14.3850 - custom_loss: 0.0263\n",
      "Epoch 26/100\n",
      "490/490 [==============================] - 140s 286ms/step - loss: 14.3852 - custom_loss: 0.0263\n",
      "Epoch 27/100\n",
      "490/490 [==============================] - 137s 280ms/step - loss: 14.3867 - custom_loss: 0.0263\n",
      "Epoch 28/100\n",
      "490/490 [==============================] - 147s 301ms/step - loss: 14.3884 - custom_loss: 0.0263\n",
      "Epoch 29/100\n",
      "490/490 [==============================] - 144s 295ms/step - loss: 14.3863 - custom_loss: 0.0263\n",
      "Epoch 30/100\n",
      "490/490 [==============================] - 140s 286ms/step - loss: 14.3866 - custom_loss: 0.0263\n",
      "Epoch 31/100\n",
      "490/490 [==============================] - 143s 292ms/step - loss: 14.3864 - custom_loss: 0.0263\n",
      "Epoch 32/100\n",
      "490/490 [==============================] - 138s 281ms/step - loss: 14.3861 - custom_loss: 0.0263\n",
      "Epoch 33/100\n",
      "490/490 [==============================] - 141s 287ms/step - loss: 14.3844 - custom_loss: 0.0263\n",
      "Epoch 34/100\n",
      "490/490 [==============================] - 140s 285ms/step - loss: 14.3869 - custom_loss: 0.0263\n",
      "Epoch 35/100\n",
      "490/490 [==============================] - 143s 292ms/step - loss: 14.3849 - custom_loss: 0.0263\n",
      "Epoch 36/100\n",
      "490/490 [==============================] - 137s 281ms/step - loss: 14.3851 - custom_loss: 0.0263\n",
      "Epoch 37/100\n",
      "490/490 [==============================] - 141s 288ms/step - loss: 14.3851 - custom_loss: 0.0263\n",
      "Epoch 38/100\n",
      "490/490 [==============================] - 141s 288ms/step - loss: 14.3874 - custom_loss: 0.0263\n",
      "Epoch 39/100\n",
      "490/490 [==============================] - 140s 286ms/step - loss: 14.3853 - custom_loss: 0.0263\n",
      "Epoch 40/100\n",
      "490/490 [==============================] - 141s 287ms/step - loss: 14.3857 - custom_loss: 0.0263\n",
      "\n",
      " Generating Summary after epoch: 39\n",
      "---->\n",
      "reports eight things passengers claim sharma authority big village medical already bollywood 30 different issued directed sushil killed resignation raised ministry resignation union n't getting election near form sushil already directed along investigation education become commission statement making passengers authorities recently deputy countries act special money received bank jd using farmers give 2017 army along 2 1 sunday decided news reported north making general called come security down through delhi other two from when had '' are it is is it of that to of that of and of of\n",
      "Epoch 41/100\n",
      "490/490 [==============================] - 148s 301ms/step - loss: 14.3858 - custom_loss: 0.0263\n",
      "Epoch 42/100\n",
      "490/490 [==============================] - 139s 284ms/step - loss: 14.3867 - custom_loss: 0.0263\n",
      "Epoch 43/100\n",
      "490/490 [==============================] - 137s 279ms/step - loss: 14.3854 - custom_loss: 0.0263\n",
      "Epoch 44/100\n",
      "490/490 [==============================] - 140s 285ms/step - loss: 14.3848 - custom_loss: 0.0263\n",
      "Epoch 45/100\n",
      "490/490 [==============================] - 137s 280ms/step - loss: 14.3860 - custom_loss: 0.0263\n",
      "Epoch 46/100\n",
      "490/490 [==============================] - 140s 285ms/step - loss: 14.3849 - custom_loss: 0.0263\n",
      "Epoch 47/100\n",
      "490/490 [==============================] - 138s 282ms/step - loss: 14.3856 - custom_loss: 0.0263\n",
      "Epoch 48/100\n",
      "490/490 [==============================] - 159s 324ms/step - loss: 14.3847 - custom_loss: 0.0263\n",
      "Epoch 49/100\n",
      "490/490 [==============================] - 137s 279ms/step - loss: 14.3850 - custom_loss: 0.0263\n",
      "Epoch 50/100\n",
      "490/490 [==============================] - 139s 283ms/step - loss: 14.3852 - custom_loss: 0.0263\n",
      "Epoch 51/100\n",
      "490/490 [==============================] - 139s 284ms/step - loss: 14.3838 - custom_loss: 0.0263\n",
      "Epoch 52/100\n",
      "490/490 [==============================] - 139s 284ms/step - loss: 14.3845 - custom_loss: 0.0263\n",
      "Epoch 53/100\n",
      "490/490 [==============================] - 137s 280ms/step - loss: 14.3845 - custom_loss: 0.0263\n",
      "Epoch 54/100\n",
      "490/490 [==============================] - 140s 285ms/step - loss: 14.3849 - custom_loss: 0.0263\n",
      "Epoch 55/100\n",
      "490/490 [==============================] - 138s 281ms/step - loss: 14.3824 - custom_loss: 0.0263\n",
      "Epoch 56/100\n",
      "490/490 [==============================] - 137s 279ms/step - loss: 14.3848 - custom_loss: 0.0263\n",
      "Epoch 57/100\n",
      "490/490 [==============================] - 149s 304ms/step - loss: 14.3848 - custom_loss: 0.0263\n",
      "Epoch 58/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490/490 [==============================] - 137s 280ms/step - loss: 14.3830 - custom_loss: 0.0263\n",
      "Epoch 59/100\n",
      "490/490 [==============================] - 140s 287ms/step - loss: 14.3834 - custom_loss: 0.0263\n",
      "Epoch 60/100\n",
      "490/490 [==============================] - 139s 285ms/step - loss: 14.3837 - custom_loss: 0.0263\n",
      "\n",
      " Generating Summary after epoch: 59\n",
      "---->\n",
      "union agency justice investigation tax north received video really 2016 education twitter railway 2 face possible raj 2017 sushil village 12 night top major lot near raj grand rajya lok war force outside call night near water making recently war sent august raised news always statement claimed ministers money election each area killed ram won election station raj airport authorities among really pradesh khan later come today crore work delhi film them from all we who we with is is with to that to to that to and to to\n",
      "Epoch 61/100\n",
      "490/490 [==============================] - 139s 284ms/step - loss: 14.3832 - custom_loss: 0.0263\n",
      "Epoch 62/100\n",
      "490/490 [==============================] - 139s 283ms/step - loss: 14.3846 - custom_loss: 0.0263\n",
      "Epoch 63/100\n",
      "490/490 [==============================] - 142s 289ms/step - loss: 14.3860 - custom_loss: 0.0263\n",
      "Epoch 64/100\n",
      "490/490 [==============================] - 142s 290ms/step - loss: 14.3835 - custom_loss: 0.0263\n",
      "Epoch 65/100\n",
      "490/490 [==============================] - 137s 279ms/step - loss: 14.3854 - custom_loss: 0.0263\n",
      "Epoch 66/100\n",
      "490/490 [==============================] - 138s 281ms/step - loss: 14.3847 - custom_loss: 0.0263\n",
      "Epoch 67/100\n",
      "490/490 [==============================] - 137s 279ms/step - loss: 14.3840 - custom_loss: 0.0263\n",
      "Epoch 68/100\n",
      "490/490 [==============================] - 140s 286ms/step - loss: 14.3842 - custom_loss: 0.0263\n",
      "Epoch 69/100\n",
      "490/490 [==============================] - 143s 292ms/step - loss: 14.3843 - custom_loss: 0.0263\n",
      "Epoch 70/100\n",
      "490/490 [==============================] - 138s 281ms/step - loss: 14.3844 - custom_loss: 0.0263\n",
      "Epoch 71/100\n",
      "490/490 [==============================] - 140s 285ms/step - loss: 14.3846 - custom_loss: 0.0263\n",
      "Epoch 72/100\n",
      "490/490 [==============================] - 141s 287ms/step - loss: 14.3840 - custom_loss: 0.0263\n",
      "Epoch 73/100\n",
      "490/490 [==============================] - 138s 283ms/step - loss: 14.3838 - custom_loss: 0.0263\n",
      "Epoch 74/100\n",
      "490/490 [==============================] - 137s 279ms/step - loss: 14.3842 - custom_loss: 0.0263\n",
      "Epoch 75/100\n",
      "490/490 [==============================] - 136s 279ms/step - loss: 14.3842 - custom_loss: 0.0263\n",
      "Epoch 76/100\n",
      "490/490 [==============================] - 139s 283ms/step - loss: 14.3837 - custom_loss: 0.0263\n",
      "Epoch 77/100\n",
      "490/490 [==============================] - 137s 280ms/step - loss: 14.3840 - custom_loss: 0.0263\n",
      "Epoch 78/100\n",
      "490/490 [==============================] - 136s 278ms/step - loss: 14.3834 - custom_loss: 0.0263\n",
      "Epoch 79/100\n",
      "490/490 [==============================] - 138s 282ms/step - loss: 14.3849 - custom_loss: 0.0263\n",
      "Epoch 80/100\n",
      "490/490 [==============================] - 138s 281ms/step - loss: 14.3835 - custom_loss: 0.0263\n",
      "\n",
      " Generating Summary after epoch: 79\n",
      "---->\n",
      "fight corporation leave release child 12 rajya sent information open ministry once inside having press yet end statement 12 video head using august sent services election started water read south education see 1 plan election using money best special himself filed august death release ministers water held others here end law without sunday head past saturday station jd 20 outside see authorities union body issue public crore man found president no if from when are who are it is is it to that the to that the and to to\n",
      "Epoch 81/100\n",
      "490/490 [==============================] - 141s 287ms/step - loss: 14.3814 - custom_loss: 0.0263\n",
      "Epoch 82/100\n",
      "490/490 [==============================] - 141s 289ms/step - loss: 14.3843 - custom_loss: 0.0263\n",
      "Epoch 83/100\n",
      "490/490 [==============================] - 139s 283ms/step - loss: 14.3840 - custom_loss: 0.0263\n",
      "Epoch 84/100\n",
      "490/490 [==============================] - 137s 280ms/step - loss: 14.3837 - custom_loss: 0.0263\n",
      "Epoch 85/100\n",
      "490/490 [==============================] - 138s 281ms/step - loss: 14.3828 - custom_loss: 0.0263\n",
      "Epoch 86/100\n",
      "490/490 [==============================] - 137s 279ms/step - loss: 14.3841 - custom_loss: 0.0263\n",
      "Epoch 87/100\n",
      "490/490 [==============================] - 142s 290ms/step - loss: 14.3839 - custom_loss: 0.0263\n",
      "Epoch 88/100\n",
      "490/490 [==============================] - 138s 281ms/step - loss: 14.3833 - custom_loss: 0.0263\n",
      "Epoch 89/100\n",
      "490/490 [==============================] - 140s 287ms/step - loss: 14.3819 - custom_loss: 0.0263\n",
      "Epoch 90/100\n",
      "490/490 [==============================] - 139s 283ms/step - loss: 14.3829 - custom_loss: 0.0263\n",
      "Epoch 91/100\n",
      "490/490 [==============================] - 137s 280ms/step - loss: 14.3834 - custom_loss: 0.0263\n",
      "Epoch 92/100\n",
      "490/490 [==============================] - 140s 285ms/step - loss: 14.3825 - custom_loss: 0.0263\n",
      "Epoch 93/100\n",
      "490/490 [==============================] - 138s 282ms/step - loss: 14.3839 - custom_loss: 0.0263\n",
      "Epoch 94/100\n",
      "490/490 [==============================] - 140s 287ms/step - loss: 14.3825 - custom_loss: 0.0263\n",
      "Epoch 95/100\n",
      "490/490 [==============================] - 136s 278ms/step - loss: 14.3826 - custom_loss: 0.0263\n",
      "Epoch 96/100\n",
      "490/490 [==============================] - 139s 283ms/step - loss: 14.3853 - custom_loss: 0.0263\n",
      "Epoch 97/100\n",
      "490/490 [==============================] - 139s 283ms/step - loss: 14.3829 - custom_loss: 0.0263\n",
      "Epoch 98/100\n",
      "490/490 [==============================] - 150s 306ms/step - loss: 14.3834 - custom_loss: 0.0263\n",
      "Epoch 99/100\n",
      "490/490 [==============================] - 138s 281ms/step - loss: 14.3825 - custom_loss: 0.0263\n",
      "Epoch 100/100\n",
      "490/490 [==============================] - 139s 283ms/step - loss: 14.3826 - custom_loss: 0.0263\n",
      "\n",
      " Generating Summary after epoch: 99\n",
      "---->\n",
      "wednesday capital late fire authorities authorities lot raised yet comes lok online member issued htshowbiz really commission killed services 1 election child among filed social received social rajya read resignation yet children already really started here raj following outside shared charges building open himself call statement lot north here lot north past working cricket must end himself raj went role due yet without maharashtra well governor taken woman found so my them be when we '' we with on was it the that the the that the of the the\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x144c1def0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#B: Summariser using Abstractive deep learning methods\n",
    "#get all sentences from data and summary to generate word2vec model\n",
    "def generate_word2vec_model(corp):\n",
    "    '''Generate word2vec model for the given corpus'''\n",
    "    sentences_for_model = []\n",
    "    for sent in corp['ctext'] + corp['text']:\n",
    "        sents = nltk.sent_tokenize(sent)\n",
    "        for sent in sents:\n",
    "            words_in_sent = []\n",
    "            for word in nltk.word_tokenize(sent):\n",
    "                if word not in punctuation:\n",
    "                    words_in_sent.append(word.lower())\n",
    "            sentences_for_model.append(words_in_sent)\n",
    "    model = gensim.models.Word2Vec(sentences_for_model, min_count=1, size=128)\n",
    "    model.save('model.embeddings')\n",
    "    corp_model = gensim.models.Word2Vec.load('model.embeddings')\n",
    "    return corp_model\n",
    "\n",
    "def word2idx(word, model):\n",
    "    '''Utility function to convert a given word to index'''\n",
    "    if(word in model.wv.vocab):\n",
    "        return model.wv.vocab[word].index\n",
    "    return 0\n",
    "\n",
    "def idx2word(idx, model):\n",
    "    '''Utility function to convert index to word'''\n",
    "    return model.wv.index2word[idx]\n",
    "\n",
    "def format_data(corp, model):\n",
    "    '''This function converts text to vector data'''\n",
    "    x_train = []\n",
    "    y_train = []\n",
    "    max_num_words_ctext = max([len(nltk.word_tokenize(sent)) for sent in corp['ctext']])\n",
    "    max_num_words_text = max([len(nltk.word_tokenize(sent)) for sent in corp['text']])\n",
    "    x_train = np.zeros([corp.shape[0], max_num_words_ctext], dtype=np.int32)\n",
    "    y_train = np.zeros([corp.shape[0], max_num_words_text], dtype=np.int32)\n",
    "    print(max_num_words_ctext)\n",
    "    print(max_num_words_text)\n",
    "    # generate vectors for article data\n",
    "    for i, text in enumerate(corp['ctext']):\n",
    "        for w, word in enumerate(nltk.word_tokenize(text)):\n",
    "            x_train[i, w] = word2idx(word.lower(), model)\n",
    "    # generate vectors for summary data        \n",
    "    for i, text in enumerate(corp['text']):\n",
    "        for w, word in enumerate(nltk.word_tokenize(text)):\n",
    "            y_train[i, w] = word2idx(word.lower(), model)\n",
    "\n",
    "    return x_train, y_train, max_num_words_text\n",
    "\n",
    "# these parameters will be used to contruct the model\n",
    "corp_model = generate_word2vec_model(df_small)\n",
    "pretrained_weights = corp_model.wv.vectors\n",
    "vocab_size, embedding_size = pretrained_weights.shape\n",
    "# standarize the word vectors so that the vectors have values between 0 and 1\n",
    "x_train, y_train, max_num_words_text = format_data(df_small, corp_model)\n",
    "x_train = x_train/vocab_size\n",
    "y_train = y_train/vocab_size\n",
    "\n",
    "validation_size = 0.2\n",
    "test_size = 0.1\n",
    "train_index = math.floor(x_train.shape[0]*(1-validation_size-test_size))\n",
    "\n",
    "x_test = x_train[train_index:]\n",
    "\n",
    "# print the orinigal article and the given summary to compare after every 20 eopchs\n",
    "x_sample = x_train[train_index:train_index+1]\n",
    "print('-----> Original article')\n",
    "print(df_small.iloc[train_index+1]['ctext'])\n",
    "print('------> Given Summary')\n",
    "print(df_small.iloc[train_index+1]['text'])\n",
    "\n",
    "x_train = x_train[:train_index]\n",
    "y_train = y_train[:train_index]\n",
    "\n",
    "def print_prediction(prediction):\n",
    "    '''This function will be executed after every 20 epochs to print the summary for a sample text '''\n",
    "    prediction = prediction*vocab_size\n",
    "    summary = []\n",
    "    for s in prediction:\n",
    "        sent = []\n",
    "        for w in s:\n",
    "            sent.append(idx2word(int(w), corp_model))\n",
    "        summary.append(' '.join(sent))\n",
    "    print('---->')\n",
    "    print('. '.join(summary))\n",
    "    return ' '.join(summary)\n",
    "\n",
    "def on_epoch_end(epoch, _):\n",
    "    '''Function call at the end of each epoch'''\n",
    "    if (epoch+1)%20 == 0:\n",
    "        print('\\n Generating Summary after epoch: %d' % epoch)\n",
    "        prediction = model.predict(x=[x_sample, x_sample])\n",
    "        print_prediction(prediction)\n",
    "\n",
    "# define a custom loss function to play around with Keras\n",
    "def custom_loss(y_true, y_pred):\n",
    "    return y_true - y_pred\n",
    "    \n",
    "\n",
    "# article input model\n",
    "# input layer\n",
    "inputs1 = Input(shape=(x_train.shape[1],))\n",
    "# embedding with pretrained weights with the news article text\n",
    "article1 = Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights])(inputs1)\n",
    "# generate LSTM model with the above embedding\n",
    "article2 = LSTM(units=embedding_size, dropout=0.05)(article1)\n",
    "# repeat vector to connect to the next layer\n",
    "article3 = RepeatVector(x_train.shape[1])(article2)\n",
    "# summary input model\n",
    "# input shape for the second input layer\n",
    "inputs2 = Input(shape=(x_train.shape[1],))\n",
    "# embedding with pretrained weights with the news article text for second input layer\n",
    "summ1 = Embedding(input_dim=vocab_size, output_dim=embedding_size, weights=[pretrained_weights])(inputs2)\n",
    "# decoder model\n",
    "# concatenate layer to connect encoder and decoder\n",
    "decoder1 = concatenate([article3, summ1])\n",
    "# LSTM model for decoder\n",
    "decoder2 = LSTM(units=embedding_size, dropout=0.05)(decoder1)\n",
    "# output layer using softmax activation\n",
    "outputs = Dense(y_train.shape[1], activation='softmax')(decoder2)\n",
    "# attention layer\n",
    "#attention = TimeDistributed(Dense(1, activation = 'tanh'))(inputs2)\n",
    "#attention = Flatten()(attention)\n",
    "#attention = Multiply()([outputs, attention])\n",
    "#attention = Activation('softmax')(attention)\n",
    "#attention = Permute([2, 1])(attention)\n",
    " \n",
    "#decoder_dense = Dense(vocab_size,activation='softmax')\n",
    "#outputs = decoder_dense(attention)\n",
    "\n",
    "# tie it together [article, summary] [word]\n",
    "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "# compile and fit the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[custom_loss])\n",
    "\n",
    "model.fit([x_train, x_train], y_train, batch_size=128, \n",
    "          epochs=100, callbacks=[LambdaCallback(on_epoch_end=on_epoch_end)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----> Original article\n",
      "in dramatic developments in bihar, nitish kumar on wednesday resigned as chief minister, dumping the rjd and congress to stitch a new alliance with bjp, which quickly announced support to a new government under him. kumar, whose resignation was immediately accepted by governor keshari nath tripathi, will take oath at the chief minister at 5pm on thursday.?in the circumstances that prevail in bihar, it became difficult to run the grand alliance government,? kumar told reporters outside raj bhavan after submitting his resignation to governor keshri nath tripathi.prime minister narendra modi hailed kumar?s resignation, saying in doing so he has joined the fight against corruption. immediately after kumar announced his resignation, modi tweeted: ?congratulations! mr nitish kumar for joining the fight against corruption.?here are wednesday?s highlights on the political crisis in bihar:12.30am: lalu prasad says: ?nitish kumar has underlined that he is an opportunist. having done what he has, he cannot stake claim without a challenge. the congress, cpi-ml and independents will back our claims to form the next government... struggle is my life. i have to be in ranchi for the next three days. however, tejashwi has been asked to present the claim to the governor in the forenoon tomorrow.?12.28am: tejashwi yadav along with rjd delegation will meet bihar governor kn tripathi at 11 am on thursday; to stake claim for forming government.12.07am: taking a letter of support to governor to stake claim for forming government: prem kumar, bjp, on sushil modi and nitish kumar?s meet with governor keshari nath tripathi.12.04am: have asked governor for time to meet, being the largest party we will stake claim to from government, tweets tejashwi yadav. we will also get support from jdu mlas, he says.11.56pm: nitish kumar, sushil modi and bjp, jdu leaders leave for raj bhavan to meet governor keshari nath tripathi.11.11pm: governor keshari nath tripathi reaches raj bhavan after being discharged from hospital.10.36pm: nitish kumar to take oath as cm of bihar at 5pm tomorrow.10.31pm: bihar governor kn tripathi admitted to indira gandhi institute of medical sciences in patna due to an ent problem.10.20pm: nitish may take oath on thursday. a meeting of jd(u) and bjp mlas is underway.10.07pm: when nitish kumar broke alliance with bjp in 2013 he choreographed and followed the same script to secure himself. he surrenders as per needs: lalu prasad?s tweet9.55pm: sushil modi says bjp has handed over a letter of support to the governor, and will be part of the new government. all nda members are heading to 1, aney marg to elect nitish kumar as the leader, modi adds.9.49pm: sushil modi, nityanand rai and other bjp mlas reach nitish kumar?s residence in patna for a meeting of jdu-bjp mlas, reports ani.9.30pm: sushil kumar modi says bjp will be part of the government under nitish kumar. bjp and jd(u) leaders are headed to the raj bhawan to meet governor keshari nath tripathi.9.02pm: ?there are charges against both nitish and tejashwi. the mlas of the grand alliance parties should sit down and pick a new leader. bihar?s poor, deprived and reservation-backed sections gave the grand alliance historic majority against the bjp. this is foeticide of the grand alliance? there were corruption allegations against us before. did nitish kumar not know while making an alliance and forming a government?? -- lalu prasad?s tweets9.01pm: nityanand rai and i called nitish ji. the bjp decides to support him; we will support the government formed under him. we will inform the governor also: ani?quotes sushil modi as saying9pm: thank pm modi for his tweet on my decision, tweets nitish kumar after resigning as bihar cm8.55pm: there?s involvement of narendra modi. nitish kumar slapped people of bihar. people like sushil modi are eager to form a government in bihar: lalu prasad8.40pm: meeting underway at lalu prasad?s residence in patna. rabri devi, tejashwi yadav, tej pratap yadav and senior rjd leaders present.8.23pm: cp joshi was informed at the governor?s house; he asked kumar to wait. joshi said he will get back after a word with rahul gandhi and sonia gandhi: ani quotes sources8.21pm: nitish kumar did not inform rahul gandhi about his intention to resign in their meeting: ani?quotes sources.8.02pm: i appeal to nitish kumar... all rjd, jd(u) and congress mlas should sit down and pick new leader and make a government... his unwillingness to do so will prove he has already struck a deal with the bjp: lalu prasad7.59pm: the bjp welcomes the battle against corruption in bihar; not in favour of midterm elections... a three-member panel from bihar will report to central leadership for final call: jp nadda after the bjp?s parliamentary board meeting.7.58pm: we are deeply disappointed by the news of nitish kumar?s resignation. the congress respects nitish kumar as a leader: randeep surjewala, congress7.57pm: nitish kumar knew that he is accused of section 302; one of india?s cm is main accused in murder and arms case : lalu prasad7.51pm: nitish kumar had said that he will never join hands with bjp: lalu prasad7.50pm: rjd chief lalu prasad says did not speak to the press over corruption allegations against  tejashwi following lawyers? advice. 7.49pm: nitish kumar did not ask for resignation: rjd chief lalu prasad at a press conference after nitish kumar quits as bihar cm.7.45pm: rjd chief lalu prasad to address a press conference shortly.7.42pm: we are happy that bihar cm didn?t compromise on issue of corruption and did not kneel down before rjd: bjp leader sushil kumar modi7.36pm: we don?t want midterm elections in bihar. the mlas should complete their term. a three-member committee will talk to mlas and apprise the centre of the situation in the state: bjp leader sushil kumar modi7.26pm: source say bjp legislature party to finalise support without preconditions at its meeting late evening today.7.25pm: nitish-lalu alliance was a mismatch. this had to happen: union minister ravi shankar prasad on nitish kumar?s resignation7.24pm: pm narendra modi arrives for bjp parliamentary meeting in delhi.7.09pm: the country and time require that for the bright future of india, and especially bihar, people need to rise above political disputes and fight against corruption together: prime minister narendra modi7.09pm: i congratulate nitish kumar for joining the fight against corruption. the 1.25 billion people of the country are welcoming and supporting his honesty: prime minister narendra modi7.08pm: before resigning, i had informed laluji and congress bihar in-charge cp joshi: nitish kumar.7.05pm: gandhiji always said needs can be met on the earth, but greed can never be fulfilled: nitish kumar7.04pm: i am not blaming anyone, those who want to blame me are free to do so: nitish kumar7.02pm: i felt that for a person like me it was not possible to run this government. i don?t want to create a dispute... when i saw there was no other way... i broke myself away (from the alliance with the rjd). the governor has accepted my resignation, and asked me to continue working till the next development.6.58pm: we worked in the alliance till the point we could: nitish kumar6.57pm: if there are charges then they must have replied to it. we felt they were not in a position to speak: nitish kumar6.56pm: it?s not possible to work in this environment; i tried. tried to find out a solution: nitish kumar6.52pm: the situation is such that it?s not possible to work. whatever we do, the discussion is only about one thing: nitish kumar6.50pm: not possible to work in the grand alliance. we made all attempts. we never asked for anyone?s resignation: nitish kumar6.34pm: bihar cm nitish kumar reaches raj bhavan to meet governor keshari nath tripathi amid speculation that he may resign.6.33pm: bihar cm nitish kumar on his way to raj bhavan to meet governor keshari nath tripathi.\n",
      "------> Given Summary\n",
      "Expressing support for Bihar CM Nitish Kumar after his resignation, PM Narendra Modi tweeted, \"Congratulations to Nitish Kumar for joining the fight against corruption.\" He further added that the 125 crore citizens of the country welcome and support his honesty. Kumar had submitted his resignation to Governor Keshari Nath Tripathi citing a crisis in the ruling Congress-RJD-JD(U) alliance.\n",
      "------> Extractive Summary\n",
      "joshi said he will get back after a word with rahul gandhi and sonia gandhi: ani quotes sources8.21pm: nitish kumar did not inform rahul gandhi about his intention to resign in their meeting: ani?quotes sources.8.02pm: i appeal to nitish kumar... all rjd, jd(u) and congress mlas should sit down and pick new leader and make a government... his unwillingness to do so will prove he has already struck a deal with the bjp: lalu prasad7.59pm: the bjp welcomes the battle against corruption in bihar; not in favour of midterm elections... a three-member panel from bihar will report to central leadership for final call: jp nadda after the bjp?s parliamentary board meeting.7.58pm: we are deeply disappointed by the news of nitish kumar?s resignation. we will also get support from jdu mlas, he says.11.56pm: nitish kumar, sushil modi and bjp, jdu leaders leave for raj bhavan to meet governor keshari nath tripathi.11.11pm: governor keshari nath tripathi reaches raj bhavan after being discharged from hospital.10.36pm: nitish kumar to take oath as cm of bihar at 5pm tomorrow.10.31pm: bihar governor kn tripathi admitted to indira gandhi institute of medical sciences in patna due to an ent problem.10.20pm: nitish may take oath on thursday. all nda members are heading to 1, aney marg to elect nitish kumar as the leader, modi adds.9.49pm: sushil modi, nityanand rai and other bjp mlas reach nitish kumar?s residence in patna for a meeting of jdu-bjp mlas, reports ani.9.30pm: sushil kumar modi says bjp will be part of the government under nitish kumar.\n",
      "------> Abstractive Summary\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---->\n",
      "wednesday capital late fire authorities authorities lot raised yet comes lok online member issued htshowbiz really commission killed services 1 election child among filed social received social rajya read resignation yet children already really started here raj following outside shared charges building open himself call statement lot north here lot north past working cricket must end himself raj went role due yet without maharashtra well governor taken woman found so my them be when we '' we with on was it the that the the that the of the the\n"
     ]
    }
   ],
   "source": [
    "# Comparison with extractive summary vs generated abstractive summary\n",
    "print('-----> Original article')\n",
    "print(df_small.iloc[train_index+1]['ctext'])\n",
    "print('------> Given Summary')\n",
    "print(df_small.iloc[train_index+1]['text'])\n",
    "print('------> Extractive Summary')\n",
    "print(df_small.iloc[train_index+1]['summary-word-freq'])\n",
    "print('------> Abstractive Summary')\n",
    "prediction = model.predict(x=[x_sample, x_sample])\n",
    "prediction = print_prediction(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "As we see above, extractive summary did a good job to summarise the content. Infact it gave more details than the actual summary. However abstractive method generated unrelated words and was unable to summarise the text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
